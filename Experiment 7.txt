Experiment 7

Aim:
Create an application using Hadoop Map Reduce.

Theory:

The Map Reduce Model:
Traditional parallel computing algorithms were developed for systems with a small number of processors, dozens rather than thousands. So it was safe to assume that processors would not fail during a computation. At significantly larger scales this assumption breaks down, as was experienced at Google in the course of having to carry out many large-scale computations similar to the one in our word counting example. The MapReduce parallel programming abstraction was developed in response to these needs, so that it could be used by many different parallel applications while leveraging a common underlying fault-tolerant implementation that was transparent to application developers. Figure 11.1 illustrates MapReduce using the word counting example where we needed to count the occurrences of each word in a collection of documents.

MapReduce proceeds in two phases, a distributed ‗map‘ operation followed by a distributed ‗reduce‘ operation; at each phase a configurable number of M ‗mapper‘ processors and R ‗reducer‘ processors are assigned to work on the problem (we have usedM = 3 and R = 2 in the illustration). The computation is coordinated by a single master process (not shown in the figure).

A MapReduce implementation of the word counting task proceeds as follows: In the map phase each mapper reads approximately 1/M th of the input (in this case documents), from the global file system, using locations given to it by the master. Each mapped then performs a‗map‘ operation to compute word frequencies for its subset of documents. These frequencies are sorted by the words they represent and written to the local file system of the mapper. At the next phase reducers are each assigned a subset of words; in our illustration the first reducer is assigned w1 and w2 while the second one handles w3 and w4. In fact during the map phase itself each mapper writes one file per reducer, based on the words assigned to each reducer, and keeps the master informed of these file locations. The master in turn informs the reducers where the partial counts for their words have been stored on the local files of respective mappers; the reducers then make remote procedure call requests to the mappers to fetch these. Each reducer performs a reduce‘ operation that sums up the frequencies for each word, which are finally written back to the GFS file system.

Procedure:

Step 1:
Open in any Browser Name Node- https://localhost:50070/

Step 2:
Open in any Browser Job Tracker- https://localhost:50030/

Step 3: 
Open hadoop/hadoop-1.2.1 → Create a document → Type something in that document and save it as test.txt
bin/hadoop fs -ls 
bin/hadoop fs -mkdir example
bin/hadoop fs -ls /user/example/
bin/hadoop fs -copyFromLocal test.txt /user/example/example
bin/hadoop jar hadoop-examples-1.2.1.jar wordcount/user/vishal/example/test.txt /hello
In Eclipse New → Java Project → Provide Project Name → Next → Select Libraries → Add Externals JARs → Go to Hadoop → hadoop-1.2.1 → Select all jar files → Again click on Add External JARs → Go to hadoop → hadoop-1.2.1 → lib → Select all JAR files → click on Finish.

Right Click on Src Folder → Select Class → Provide a Class name: WCE → Package name: com.WordCount.Example → Click on Finish.

import java.io.IOException; 
import java.util.*;
import org.apache.hadoop.fs.Path; 
import org.apache.hadoop.conf.*; 
import org.apache.hadoop.io.*; 
import org.apache.hadoop.mapred.*; 
import org.apache.hadoop.util.*;

public class WCE

{
	public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, IntWritable>
	{
		private final static IntWritable one = new IntWritable(1); 
		private Text word = new Text();
		public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException
		{
			String line = value.toString();
			StringTokenizer tokenizer = new StringTokenizer(line); 
			While (tokenizer.hasMoreTokens())
			{
				word.set(tokenizer.nextToken()); 
				output.collect(word, one);
			}
		}
	}

	public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable>
	{
		public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException 
		{
			int sum = 0;
			while (values.hasNext())
			{
				sum += values.next().get();
			}
		output.collect(key, new IntWritable(sum));
		}
	}

	public static void main(String[] args) throws Exception
	{ 
		JobConf conf = new JobConf(WCE.class); 
		conf.setJobName("wordcount"); 
		conf.setOutputKeyClass(Text.class); 
		conf.setOutputValueClass(IntWritable.class); 
		conf.setMapperClass(Map.class); 
		conf.setCombinerClass(Reduce.class);
		conf.setReducerClass(Reduce.class); 
		FileInputFormat.addInputPath(conf, new Path(args[0])); 
		FileOutputFormat.setOutputPath(conf, new Path(args[1])); 
		JobClient.runJob(conf);
	}
}

Right Click on Project Name → New → File → Sample → type something in the sample file.
 
Right Click on Project Name → Export → Click on Java → JAR File → Provide a JAR File Name → Select the Location where to save the JAR file.

Right Click on Project Name → Run as → Run Configuration → Java Application → New → In Main → WordCount → Click on Search and click on the JAR File which you have created → Click on Arguments → Provide under Program arguments → sample output Click on Run.

Right Click on Project Name → Refresh → An output file is created in your project.

Conclusion: 
Hence we have implemented Map Reduce example such as Word Count program on a file which will count the number of times a word repeats in the given file.
